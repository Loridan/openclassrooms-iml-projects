{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artificial-audience",
   "metadata": {},
   "source": [
    "**VGG16 Transfer Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-socket",
   "metadata": {},
   "source": [
    "[VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION arxiv.org/pdf](https://arxiv.org/pdf/1409.1556.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-williams",
   "metadata": {},
   "source": [
    "During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image.\n",
    "The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel.\n",
    "The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very\n",
    "small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down,\n",
    "center)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-armor",
   "metadata": {},
   "source": [
    "**Import des librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "matched-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neutral-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-official",
   "metadata": {},
   "source": [
    "**Parametres CUDA pour modélisation en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legislative-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de GPU disponible :  1\n"
     ]
    }
   ],
   "source": [
    "# Installer CUDA, CUDNN\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"Nombre de GPU disponible : \", len(gpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-queen",
   "metadata": {},
   "source": [
    "**Parametres divers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "operational-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "N_EPOCHS = 150\n",
    "\n",
    "ADAM_LEARNING_RATE = 0.0003\n",
    "PATIENCE_ES = 12\n",
    "\n",
    "VALIDATION_RATIO = 0.2\n",
    "TEST_RATIO = 0.1\n",
    "TRAIN_RATIO = 1 - VALIDATION_RATIO - TEST_RATIO\n",
    "\n",
    "N_BREEDS = 20\n",
    "N_IMAGE_PER_CLASS = 140\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-travel",
   "metadata": {},
   "source": [
    "**On fixe la randomness pour la répétabilité de l'experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ambient-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-inside",
   "metadata": {},
   "source": [
    "**Préparation des dossiers pour la génération d'images et l'augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optimum-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se déplace dans le dossier images\n",
    "os.chdir('data/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subsequent-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprimer les dossiers de modélisations et leurs contenus si déjà éxistants dans images\n",
    "_ = [shutil.rmtree(path) for path in [\"train\",\"valid\",\"test\"] if os.path.isdir(path) is True ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wrong-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recuperer les races (subdirectories)\n",
    "list_dir_breeds = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "civic-dream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race</th>\n",
       "      <th>nombre_images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02085620-Chihuahua</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02085782-Japanese_spaniel</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n02085936-Maltese_dog</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n02086079-Pekinese</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n02086240-Shih-Tzu</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>n02113799-standard_poodle</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>n02113978-Mexican_hairless</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>n02115641-dingo</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>n02115913-dhole</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>n02116738-African_hunting_dog</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         race  nombre_images\n",
       "0    n02085620-Chihuahua                                 152\n",
       "1    n02085782-Japanese_spaniel                          185\n",
       "2    n02085936-Maltese_dog                               252\n",
       "3    n02086079-Pekinese                                  149\n",
       "4    n02086240-Shih-Tzu                                  214\n",
       "..                                        ...            ...\n",
       "115  n02113799-standard_poodle                           159\n",
       "116  n02113978-Mexican_hairless                          155\n",
       "117  n02115641-dingo                                     156\n",
       "118  n02115913-dhole                                     150\n",
       "119  n02116738-African_hunting_dog                       169\n",
       "\n",
       "[120 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On observe le nombre d'images pour chaque races dans leur dossier respectifs à l'aide d'un dataframe\n",
    "df_breds = pd.DataFrame([[f\"{path:40}\",len(os.listdir(path))] for path in list_dir_breeds] , columns=[\"race\", \"nombre_images\"])\n",
    "df_breds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "equal-genetics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n02106166-Border_collie',\n",
       " 'n02089078-black-and-tan_coonhound',\n",
       " 'n02086079-Pekinese',\n",
       " 'n02108915-French_bulldog',\n",
       " 'n02094258-Norwich_terrier',\n",
       " 'n02093754-Border_terrier',\n",
       " 'n02093256-Staffordshire_bullterrier',\n",
       " 'n02090379-redbone',\n",
       " 'n02088632-bluetick',\n",
       " 'n02107312-miniature_pinscher',\n",
       " 'n02113712-miniature_poodle',\n",
       " 'n02102480-Sussex_spaniel',\n",
       " 'n02088364-beagle',\n",
       " 'n02105251-briard',\n",
       " 'n02099267-flat-coated_retriever',\n",
       " 'n02086240-Shih-Tzu',\n",
       " 'n02092339-Weimaraner',\n",
       " 'n02093428-American_Staffordshire_terrier',\n",
       " 'n02101388-Brittany_spaniel',\n",
       " 'n02105505-komondor']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On selectionne au hasard un nombre N de races.\n",
    "list_dir_breeds = random.sample(list_dir_breeds, N_BREEDS)\n",
    "list_dir_breeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-raising",
   "metadata": {},
   "source": [
    "Pour éviter d'augmenter les données lors de la séparation du jeu de validation de Keras avec ImageDataGenerator, il y a plusieurs techniques, je vais séparer le jeu puis créer différent ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "excess-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organiser les données en un dossier d'entrainement, de validation et de test\n",
    "for dir_breeds in list_dir_breeds:\n",
    "    path_train = f\"train/{dir_breeds}\"\n",
    "    path_valid = f\"valid/{dir_breeds}\"\n",
    "    path_test = f\"test/{dir_breeds}\"\n",
    "    \n",
    "    # On crée nos dossiers vides\n",
    "    [os.makedirs(path) for path in [path_train,path_valid,path_test] if os.path.isdir(path) is False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acknowledged-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si le nombre d'images minimum par classe est inférieur à notre paramétre on renvoit une erreur\n",
    "min_images = df_breds[\"nombre_images\"].min()\n",
    "assert(min_images > N_IMAGE_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "appropriate-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, valid_size, test_size = [int(TRAIN_RATIO*N_IMAGE_PER_CLASS) , int(VALIDATION_RATIO*N_IMAGE_PER_CLASS), int(TEST_RATIO*N_IMAGE_PER_CLASS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "internal-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "for dir_breeds in list_dir_breeds:\n",
    "    path_train = f\"train/{dir_breeds}\"\n",
    "    path_valid = f\"valid/{dir_breeds}\"\n",
    "    path_test = f\"test/{dir_breeds}\"\n",
    "    \n",
    "    # Si nos dossiers sont vides\n",
    "    if len(os.listdir(path_train)+os.listdir(path_valid)+os.listdir(path_test)) == 0:\n",
    "\n",
    "        list_path_images = os.listdir(path=dir_breeds)\n",
    "        \n",
    "        # On ajoute le nombre d'images choisi pour l'entrainement, la validation et le test\n",
    "        for path_image in random.sample(list_path_images, train_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_train)\n",
    "            list_path_images.remove(path_image)\n",
    "\n",
    "        for path_image in random.sample(list_path_images, valid_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_valid)\n",
    "            list_path_images.remove(path_image)\n",
    "\n",
    "        for path_image in random.sample(list_path_images, test_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_test)\n",
    "            list_path_images.remove(path_image)\n",
    "    else:\n",
    "        print(\"Les dossiers ne sont pas vides\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cordless-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On revient dans le dossier root\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "configured-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins de nos dossiers fraichement générer\n",
    "train_path = \"data/images/train\"\n",
    "valid_path = \"data/images/valid\"\n",
    "test_path = \"data/images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "molecular-tutorial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches :\n",
      "Found 1960 images belonging to 20 classes.\n",
      "valid batches :\n",
      "Found 560 images belonging to 20 classes.\n",
      "test batches :\n",
      "Found 280 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "# Création des itérateurs de données pour notre modélisation\n",
    "print(\"train batches :\")\n",
    "train_image_data_generator = ImageDataGenerator(rotation_range=40,\n",
    "                                                width_shift_range=0.2,\n",
    "                                                height_shift_range=0.2,\n",
    "                                                shear_range=0.2,\n",
    "                                                zoom_range=0.2,\n",
    "                                                horizontal_flip=True,\n",
    "                                                preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "train_batches = train_image_data_generator.flow_from_directory(directory=train_path,\n",
    "                                                               target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                               classes=list_dir_breeds,\n",
    "                                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"valid batches :\")\n",
    "valid_image_data_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "valid_batches = valid_image_data_generator.flow_from_directory(directory=valid_path,\n",
    "                                                               target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                               classes=list_dir_breeds,\n",
    "                                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(\"test batches :\")\n",
    "test_image_data_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "test_batches = test_image_data_generator.flow_from_directory(directory=test_path,\n",
    "                                                             target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                             classes=list_dir_breeds,\n",
    "                                                             batch_size=BATCH_SIZE,\n",
    "                                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-consent",
   "metadata": {},
   "source": [
    "**Visualisation de l'augmentation avant entrainement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génére le prochain batch du train avec augmentation\n",
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction custom pour visualiser l'augmentation\n",
    "def plot_images(images_arr, labels=None, rescaled=True,print_shape=True):\n",
    "    if rescaled:\n",
    "        images_arr= images_arr*255.0\n",
    "    size = len(images_arr)\n",
    "    fig, axes = plt.subplots(1, size, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for i, (img, ax) in enumerate(zip( images_arr, axes)):\n",
    "        img = img.astype(np.uint8)\n",
    "        ax.imshow(img)\n",
    "        if labels is not None:\n",
    "            ax.set_title(labels[i])\n",
    "        elif print_shape is True :\n",
    "            ax.set_title(np.array(img).shape)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(imgs, print_shape=True, rescaled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-patio",
   "metadata": {},
   "source": [
    "**Définir les paramétres du gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les hyperparametres\n",
    "HP_ADAM_LEARNING_RATE = hp.HParam(\"adam_learning_rate\", hp.RealInterval(0.005, 0.01))\n",
    "\n",
    "METRIC_ACCURACY = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-documentation",
   "metadata": {},
   "source": [
    "**Création et configuration des fichiers dans tensorboard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/hparam_tuning_\" + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams = [HP_ADAM_LEARNING_RATE,\n",
    "                  ],\n",
    "        metrics = [hp.Metric(METRIC_ACCURACY,\n",
    "                             display_name=\"Accuracy\")\n",
    "                  ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-consultation",
   "metadata": {},
   "source": [
    "**Fonctions pour la modélisation avec gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de modélisation avec les hyperparametres fournis\n",
    "\n",
    "def model(hparams) :\n",
    "    \n",
    "    # On récupere le modele avec les poids entrainés\n",
    "    model = VGG16(weights = \"imagenet\",\n",
    "                  include_top = True\n",
    "                 )\n",
    "    \n",
    "    # On crée notre derniere couche à partir des couches précédentes de vgg16\n",
    "    last_layer = Dense(units = N_BREEDS,\n",
    "                       activation ='softmax',\n",
    "                       name ='predictions'\n",
    "                      )(model.layers[-2].output)\n",
    "\n",
    "    # Puis on crée notre modéle\n",
    "    model = tf.keras.Model(inputs = model.input,\n",
    "                           outputs = last_layer\n",
    "                          )\n",
    "    \n",
    "    # freeze all\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # unfreeze avec poids non aléatoire (experimentation)\n",
    "    model.layers[-1].trainable = True\n",
    "    model.layers[-2].trainable = True\n",
    "    model.layers[-3].trainable = True\n",
    "    \n",
    "    # On compile\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=ADAM_LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy']\n",
    "                 )\n",
    "    \n",
    "    # Définir les callbacks\n",
    "    my_callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),\n",
    "        hp.KerasCallback(log_dir, hparams),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = PATIENCE_ES),\n",
    "        tf.keras.callbacks.ModelCheckpoint(f\"model_VGG16_{N_BREEDS}_{'_'.join(str(x) for x in list(hparams.values()))}.hdf5\",\n",
    "                                           save_best_only=True,\n",
    "                                           monitor='val_loss',\n",
    "                                           mode='min')\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # On entraine, on récupere l'historique et on enregistre dans nos logs\n",
    "    history = model.fit(x = train_batches,\n",
    "                        steps_per_epoch = len(train_batches),\n",
    "                        validation_data = valid_batches,\n",
    "                        validation_steps = len(valid_batches),\n",
    "                        epochs = N_EPOCHS,\n",
    "                        verbose = 2,\n",
    "                        callbacks=my_callbacks\n",
    "                       )\n",
    "    \n",
    "    # On retourne la métrique d'optimisation\n",
    "    return history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'enregistrement pour chaque appel à notre fonction de modélisation, des hyperparametres utilisés et de la métrique\n",
    "\n",
    "def run(run_dir, hparams) :\n",
    "    \n",
    "    with tf.summary.create_file_writer(run_dir).as_default() :\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = model(hparams)\n",
    "        \n",
    "        # On convertit notre tensor métrique en scalaire\n",
    "        accuracy= tf.reshape(tf.convert_to_tensor(accuracy), []).numpy()\n",
    "        \n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher commande a executer pour tensorboard\n",
    "f\"python -m tensorboard.main --logdir=\\\"{log_dir}\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-highland",
   "metadata": {},
   "source": [
    "**Modélisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lance l'optimisation du modele avec notre espace d'hyperparametres\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "for adam_learning_rate in tf.linspace(HP_ADAM_LEARNING_RATE.domain.min_value,\n",
    "                                      HP_ADAM_LEARNING_RATE.domain.max_value,\n",
    "                                      1\n",
    "                                     ).numpy():\n",
    "    hparams = {\n",
    "        HP_ADAM_LEARNING_RATE: adam_learning_rate\n",
    "    }\n",
    "    run_name = f\"/run-{session_num}\"\n",
    "    run_dir = log_dir + run_name\n",
    "    print(f\"--- Starting trial:{run_name}\")\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    run(run_dir, hparams)\n",
    "    session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-treaty",
   "metadata": {},
   "source": [
    "**Visualisation des résultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-cannon",
   "metadata": {},
   "source": [
    "ou pour afficher le tableau de bord en cours, on lance la commande suivante avec le chemin du répertoire où les différents journaux d'exécution ont été stockés:\n",
    "\n",
    "python -m tensorboard.main --logdir = log_dir\n",
    "\n",
    "et pour c\n",
    "taskkill /IM \"tensorboard.exe\" /F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-grave",
   "metadata": {},
   "source": [
    "**Matrice de confusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "secondary-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = dict(test_batches.class_indices.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hired-mining",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'n02106166-Border_collie',\n",
       " 1: 'n02089078-black-and-tan_coonhound',\n",
       " 2: 'n02086079-Pekinese',\n",
       " 3: 'n02108915-French_bulldog',\n",
       " 4: 'n02094258-Norwich_terrier',\n",
       " 5: 'n02093754-Border_terrier',\n",
       " 6: 'n02093256-Staffordshire_bullterrier',\n",
       " 7: 'n02090379-redbone',\n",
       " 8: 'n02088632-bluetick',\n",
       " 9: 'n02107312-miniature_pinscher',\n",
       " 10: 'n02113712-miniature_poodle',\n",
       " 11: 'n02102480-Sussex_spaniel',\n",
       " 12: 'n02088364-beagle',\n",
       " 13: 'n02105251-briard',\n",
       " 14: 'n02099267-flat-coated_retriever',\n",
       " 15: 'n02086240-Shih-Tzu',\n",
       " 16: 'n02092339-Weimaraner',\n",
       " 17: 'n02093428-American_Staffordshire_terrier',\n",
       " 18: 'n02101388-Brittany_spaniel',\n",
       " 19: 'n02105505-komondor'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v: k for k, v in dico.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(generator, model, figsize=(15,15)):\n",
    "    n_steps = len(generator)\n",
    "    y_true = None\n",
    "    y_pred = None\n",
    "\n",
    "    # evaluation\n",
    "    for step in range(n_steps):\n",
    "        imgs, labels = next(generator)\n",
    "        preds = model.predict(imgs)\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        if y_true is None:\n",
    "            y_true = labels\n",
    "        if y_pred is None:\n",
    "            y_pred = preds\n",
    "        else:\n",
    "            y_true = np.concatenate((y_true, labels))\n",
    "            y_pred = np.concatenate((y_pred, preds))\n",
    "\n",
    "    y_pred = y_pred.astype(np.float64)\n",
    "    y_true = y_true.astype(np.float64)\n",
    "\n",
    "    # conversion inverse pour multiclass\n",
    "    categories = list(generator.class_indices.keys())\n",
    "    categories_idx = [[element] for element in list(generator.class_indices.values())]\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    onehot_encoder.fit(categories_idx)\n",
    "    y_true = onehot_encoder.inverse_transform(y_true)\n",
    "    y_true = [element[0] for element in y_true]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cm = ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred),\n",
    "                           display_labels=categories\n",
    "                          ).plot(ax=ax,\n",
    "                                 colorbar=None,\n",
    "                                 cmap=plt.cm.Blues)\n",
    "    \n",
    "    # ameliore l'affichage des labels en pivotants\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_ha(\"right\")\n",
    "        label.set_rotation(45)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model\n",
    "model_path = 'model_VGG16_3_0.001.hdf5'\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_batches, model, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-fitness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

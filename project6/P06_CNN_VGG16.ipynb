{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "plain-armor",
   "metadata": {},
   "source": [
    "**Import des librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-official",
   "metadata": {},
   "source": [
    "**Parametres CUDA pour modélisation en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer CUDA, CUDNN\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"Nombre de GPU disponible : \", len(gpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-queen",
   "metadata": {},
   "source": [
    "**Parametres divers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "N_EPOCHS = 150\n",
    "\n",
    "ADAM_LEARNING_RATE = 0.0003\n",
    "PATIENCE_ES = 12\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "VALIDATION_RATIO = 0.2\n",
    "TEST_RATIO = 0.1\n",
    "TRAIN_RATIO = 1 - VALIDATION_RATIO - TEST_RATIO\n",
    "\n",
    "N_BREEDS = 20\n",
    "N_IMAGE_PER_CLASS = 140\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-inside",
   "metadata": {},
   "source": [
    "**Préparation des dossiers pour la génération d'images et l'augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se déplace dans le dossier images\n",
    "os.chdir('data/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprimer les dossiers de modélisations et leurs contenus si déjà éxistants dans images\n",
    "_ = [shutil.rmtree(path) for path in [\"train\",\"valid\",\"test\"] if os.path.isdir(path) is True ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recuperer les races (subdirectories)\n",
    "list_dir_breeds = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On observe le nombre d'images pour chaque races dans leur dossier respectifs à l'aide d'un dataframe\n",
    "df_breds = pd.DataFrame([[f\"{path:40}\",len(os.listdir(path))] for path in list_dir_breeds] , columns=[\"race\", \"nombre_images\"])\n",
    "df_breds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On selectionne au hasard un nombre N de races.\n",
    "list_dir_breeds = random.sample(list_dir_breeds, N_BREEDS)\n",
    "list_dir_breeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-raising",
   "metadata": {},
   "source": [
    "Pour éviter d'augmenter les données lors de la séparation du jeu de validation de Keras avec ImageDataGenerator, il y a plusieurs techniques, je vais séparer le jeu puis créer différent ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organiser les données en un dossier d'entrainement, de validation et de test\n",
    "for dir_breeds in list_dir_breeds:\n",
    "    path_train = f\"train/{dir_breeds}\"\n",
    "    path_valid = f\"valid/{dir_breeds}\"\n",
    "    path_test = f\"test/{dir_breeds}\"\n",
    "    \n",
    "    # On crée nos dossiers vides\n",
    "    [os.makedirs(path) for path in [path_train,path_valid,path_test] if os.path.isdir(path) is False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si le nombre d'images minimum par classe est inférieur à notre paramétre on renvoit une erreur\n",
    "min_images = df_breds[\"nombre_images\"].min()\n",
    "assert(min_images > N_IMAGE_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, valid_size, test_size = [int(TRAIN_RATIO*N_IMAGE_PER_CLASS) , int(VALIDATION_RATIO*N_IMAGE_PER_CLASS), int(TEST_RATIO*N_IMAGE_PER_CLASS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "for dir_breeds in list_dir_breeds:\n",
    "    path_train = f\"train/{dir_breeds}\"\n",
    "    path_valid = f\"valid/{dir_breeds}\"\n",
    "    path_test = f\"test/{dir_breeds}\"\n",
    "    \n",
    "    # Si nos dossiers sont vides\n",
    "    if len(os.listdir(path_train)+os.listdir(path_valid)+os.listdir(path_test)) == 0:\n",
    "\n",
    "        list_path_images = os.listdir(path=dir_breeds)\n",
    "        \n",
    "        # On ajoute le nombre d'images choisi pour l'entrainement, la validation et le test\n",
    "        for path_image in random.sample(list_path_images, train_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_train)\n",
    "            list_path_images.remove(path_image)\n",
    "\n",
    "        for path_image in random.sample(list_path_images, valid_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_valid)\n",
    "            list_path_images.remove(path_image)\n",
    "\n",
    "        for path_image in random.sample(list_path_images, test_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_test)\n",
    "            list_path_images.remove(path_image)\n",
    "    else:\n",
    "        print(\"Les dossiers ne sont pas vides\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On revient dans le dossier root\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins de nos dossiers fraichement générer\n",
    "train_path = \"data/images/train\"\n",
    "valid_path = \"data/images/valid\"\n",
    "test_path = \"data/images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des itérateurs de données pour notre modélisation\n",
    "print(\"train batches :\")\n",
    "train_image_data_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                                rotation_range=40,\n",
    "                                                width_shift_range=0.2,\n",
    "                                                height_shift_range=0.2,\n",
    "                                                shear_range=0.2,\n",
    "                                                zoom_range=0.2,\n",
    "                                                horizontal_flip=True)\n",
    "train_batches = train_image_data_generator.flow_from_directory(directory=train_path,\n",
    "                                                               target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                               classes=list_dir_breeds,\n",
    "                                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"valid batches :\")\n",
    "valid_image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "valid_batches = valid_image_data_generator.flow_from_directory(directory=valid_path,\n",
    "                                                               target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                               classes=list_dir_breeds,\n",
    "                                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(\"test batches :\")\n",
    "test_image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_batches = test_image_data_generator.flow_from_directory(directory=test_path,\n",
    "                                                             target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                             classes=list_dir_breeds,\n",
    "                                                             batch_size=BATCH_SIZE,\n",
    "                                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-swiss",
   "metadata": {},
   "source": [
    "**Analyse du nombre d'images par race**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_breds, orientation='h', y='race', x=\"nombre_images\", width=800, height=800)\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-consent",
   "metadata": {},
   "source": [
    "**Visualisation de l'augmentation avant entrainement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génére le prochain batch du train avec augmentation\n",
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction custom pour visualiser l'augmentation\n",
    "def plot_images(images_arr, labels=None, rescaled=True,print_shape=True):\n",
    "    if rescaled:\n",
    "        images_arr= images_arr*255.0\n",
    "    size = len(images_arr)\n",
    "    fig, axes = plt.subplots(1, size, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for i, (img, ax) in enumerate(zip( images_arr, axes)):\n",
    "        img = img.astype(np.uint8)\n",
    "        ax.imshow(img)\n",
    "        if labels is not None:\n",
    "            ax.set_title(labels[i])\n",
    "        elif print_shape is True :\n",
    "            ax.set_title(np.array(img).shape)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(imgs, print_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-highland",
   "metadata": {},
   "source": [
    "**Modélisation avec CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters = 32,\n",
    "           kernel_size = (3, 3),\n",
    "           activation = 'relu',\n",
    "           padding = 'same',\n",
    "           input_shape = (IMG_HEIGHT,IMG_WIDTH,3)\n",
    "          ),\n",
    "    MaxPool2D(pool_size = (2, 2),\n",
    "              strides=2\n",
    "             ),\n",
    "    Conv2D(filters = 64,\n",
    "           kernel_size=(3, 3),\n",
    "           activation = 'relu',\n",
    "           padding = 'same'\n",
    "          ),\n",
    "    MaxPool2D(pool_size = (2, 2),\n",
    "              strides = 2\n",
    "             ),\n",
    "    Flatten(),\n",
    "    Dense(units = 160,\n",
    "          activation = 'relu'\n",
    "         ),\n",
    "    Dense(units = N_BREEDS,\n",
    "          activation = 'softmax'\n",
    "         )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate=ADAM_LEARNING_RATE),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n",
    "                                     patience = PATIENCE_ES\n",
    "                                    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = train_batches,\n",
    "                    steps_per_epoch = len(train_batches),\n",
    "                    validation_data = valid_batches,\n",
    "                    validation_steps = len(valid_batches),\n",
    "                    epochs = N_EPOCHS,\n",
    "                    verbose = 2,\n",
    "                    callbacks = my_callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Afficher l'évolution par epoch\n",
    "def plot_evolution(history):\n",
    "    plt.plot(history.history[\"accuracy\"])\n",
    "    plt.plot(history.history[\"val_accuracy\"])\n",
    "    plt.title(f\"Evolution de l'accuracy pour {N_BREEDS} races\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.legend([\"train\", \"val\"], loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-sydney",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-discretion",
   "metadata": {},
   "source": [
    "**Transfert Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-rendering",
   "metadata": {},
   "source": [
    "[VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION arxiv.org/pdf](https://arxiv.org/pdf/1409.1556.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-pierre",
   "metadata": {},
   "source": [
    "During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image.\n",
    "The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel.\n",
    "The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very\n",
    "small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down,\n",
    "center).\n",
    "In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as\n",
    "a linear transformation of the input channels (followed by non-linearity). The convolution stride is\n",
    "fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved\n",
    "after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by\n",
    "five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed\n",
    "by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.\n",
    "A stack of convolutional layers (which has a different depth in different architectures) is followed by\n",
    "three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-\n",
    "way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is\n",
    "the soft-max layer. The configuration of the fully connected layers is the same in all networks.\n",
    "All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity.\n",
    "We note that none of our networks (except for one) contain Local Response Normalisation\n",
    "(LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation\n",
    "does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those\n",
    "of (Krizhevsky et al., 2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupere le modele avec les poids entrainés\n",
    "vgg16 = VGG16(weights = \"imagenet\",\n",
    "              include_top = True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée notre derniere couche à partir des couches précédentes de vgg16\n",
    "last_layer = Dense(units = N_BREEDS,\n",
    "                   activation ='softmax',\n",
    "                   name ='predictions'\n",
    "                  )(vgg16.layers[-2].output)\n",
    "\n",
    "# Puis on crée notre modéle\n",
    "model = tf.keras.Model(inputs = vgg16.input,\n",
    "                       outputs = last_layer\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.layers[-1].trainable = True\n",
    "model.layers[-2].trainable = True\n",
    "model.layers[-3].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-irrigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = Adam(learning_rate=0.0003),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\",\n",
    "                                     patience = 12\n",
    "                                    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = train_batches,\n",
    "                    steps_per_epoch = len(train_batches),\n",
    "                    validation_data = valid_batches,\n",
    "                    validation_steps = len(valid_batches),\n",
    "                    epochs = N_EPOCHS,\n",
    "                    verbose = 2,\n",
    "                    callbacks = my_callbacks\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evolution(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

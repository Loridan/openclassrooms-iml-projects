{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alternate-thursday",
   "metadata": {},
   "source": [
    "**Mon CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-armor",
   "metadata": {},
   "source": [
    "**Import des librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-official",
   "metadata": {},
   "source": [
    "**Parametres CUDA pour modélisation en local**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer CUDA, CUDNN\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(\"Nombre de GPU disponible : \", len(gpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-queen",
   "metadata": {},
   "source": [
    "**Parametres divers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-prophet",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "N_EPOCHS = 150\n",
    "\n",
    "VALIDATION_RATIO = 0.2\n",
    "TEST_RATIO = 0.1\n",
    "TRAIN_RATIO = 1 - VALIDATION_RATIO - TEST_RATIO\n",
    "\n",
    "N_BREEDS = 3\n",
    "N_IMAGE_PER_CLASS = 140\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-travel",
   "metadata": {},
   "source": [
    "**On fixe la randomness pour la répétabilité de l'experience**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-inside",
   "metadata": {},
   "source": [
    "**Préparation des dossiers pour la génération d'images et l'augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se déplace dans le dossier images\n",
    "os.chdir('data/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprimer les dossiers de modélisations et leurs contenus si déjà éxistants dans images\n",
    "_ = [shutil.rmtree(path) for path in [\"train\",\"valid\",\"test\"] if os.path.isdir(path) is True ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On recuperer les races (subdirectories)\n",
    "list_dir_breeds = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On observe le nombre d'images pour chaque races dans leur dossier respectifs à l'aide d'un dataframe\n",
    "df_breds = pd.DataFrame([[f\"{path:40}\",len(os.listdir(path))] for path in list_dir_breeds] , columns=[\"race\", \"nombre_images\"])\n",
    "df_breds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On selectionne au hasard un nombre N de races.\n",
    "list_dir_breeds = random.sample(list_dir_breeds, N_BREEDS)\n",
    "list_dir_breeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-raising",
   "metadata": {},
   "source": [
    "Pour éviter d'augmenter les données lors de la séparation du jeu de validation de Keras avec ImageDataGenerator, il y a plusieurs techniques, je vais séparer le jeu puis créer différent ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organiser les données en un dossier d'entrainement, de validation et de test\n",
    "for dir_breeds in list_dir_breeds:\n",
    "    path_train = f\"train/{dir_breeds}\"\n",
    "    path_valid = f\"valid/{dir_breeds}\"\n",
    "    path_test = f\"test/{dir_breeds}\"\n",
    "    \n",
    "    # On crée nos dossiers vides\n",
    "    [os.makedirs(path) for path in [path_train,path_valid,path_test] if os.path.isdir(path) is False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si le nombre d'images minimum par classe est inférieur à notre paramétre on renvoit une erreur\n",
    "min_images = df_breds[\"nombre_images\"].min()\n",
    "assert(min_images > N_IMAGE_PER_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, valid_size, test_size = [int(TRAIN_RATIO*N_IMAGE_PER_CLASS) , int(VALIDATION_RATIO*N_IMAGE_PER_CLASS), int(TEST_RATIO*N_IMAGE_PER_CLASS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "for dir_breeds in list_dir_breeds:\n",
    "    path_train = f\"train/{dir_breeds}\"\n",
    "    path_valid = f\"valid/{dir_breeds}\"\n",
    "    path_test = f\"test/{dir_breeds}\"\n",
    "    \n",
    "    # Si nos dossiers sont vides\n",
    "    if len(os.listdir(path_train)+os.listdir(path_valid)+os.listdir(path_test)) == 0:\n",
    "\n",
    "        list_path_images = os.listdir(path=dir_breeds)\n",
    "        \n",
    "        # On ajoute le nombre d'images choisi pour l'entrainement, la validation et le test\n",
    "        for path_image in random.sample(list_path_images, train_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_train)\n",
    "            list_path_images.remove(path_image)\n",
    "\n",
    "        for path_image in random.sample(list_path_images, valid_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_valid)\n",
    "            list_path_images.remove(path_image)\n",
    "\n",
    "        for path_image in random.sample(list_path_images, test_size):\n",
    "            shutil.copy(f\"{dir_breeds}/{path_image}\", path_test)\n",
    "            list_path_images.remove(path_image)\n",
    "    else:\n",
    "        print(\"Les dossiers ne sont pas vides\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On revient dans le dossier root\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemins de nos dossiers fraichement générer\n",
    "train_path = \"data/images/train\"\n",
    "valid_path = \"data/images/valid\"\n",
    "test_path = \"data/images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des itérateurs de données pour notre modélisation\n",
    "print(\"train batches :\")\n",
    "train_image_data_generator = ImageDataGenerator(rescale=1./255,\n",
    "                                                rotation_range=40,\n",
    "                                                width_shift_range=0.2,\n",
    "                                                height_shift_range=0.2,\n",
    "                                                shear_range=0.2,\n",
    "                                                zoom_range=0.2,\n",
    "                                                horizontal_flip=True)\n",
    "train_batches = train_image_data_generator.flow_from_directory(directory=train_path,\n",
    "                                                               target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                               classes=list_dir_breeds,\n",
    "                                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"valid batches :\")\n",
    "valid_image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "valid_batches = valid_image_data_generator.flow_from_directory(directory=valid_path,\n",
    "                                                               target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                               classes=list_dir_breeds,\n",
    "                                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(\"test batches :\")\n",
    "test_image_data_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_batches = test_image_data_generator.flow_from_directory(directory=test_path,\n",
    "                                                             target_size=(IMG_HEIGHT,IMG_WIDTH),\n",
    "                                                             classes=list_dir_breeds,\n",
    "                                                             batch_size=BATCH_SIZE,\n",
    "                                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-consent",
   "metadata": {},
   "source": [
    "**Visualisation de l'augmentation avant entrainement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On génére le prochain batch du train avec augmentation\n",
    "imgs, labels = next(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction custom pour visualiser l'augmentation\n",
    "def plot_images(images_arr, labels=None, rescaled=True,print_shape=True):\n",
    "    if rescaled:\n",
    "        images_arr= images_arr*255.0\n",
    "    size = len(images_arr)\n",
    "    fig, axes = plt.subplots(1, size, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for i, (img, ax) in enumerate(zip( images_arr, axes)):\n",
    "        img = img.astype(np.uint8)\n",
    "        ax.imshow(img)\n",
    "        if labels is not None:\n",
    "            ax.set_title(labels[i])\n",
    "        elif print_shape is True :\n",
    "            ax.set_title(np.array(img).shape)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(imgs, print_shape=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-patio",
   "metadata": {},
   "source": [
    "**Définir les paramétres du gridsearch**\n",
    "\n",
    "- Num of units in the first Dense layer: 256 and 512\n",
    "- Drop out rate: the range is between 0.1 and 0.2. So a dropout rate of 0.1 and 0.2 will be used.\n",
    "- Optimizers: adam, SGD, and rmsprop\n",
    "- Learning rate for the optimizers:0.001, 0.0001 and 0.0005,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-choir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les hyperparametres\n",
    "HP_CONV_FILTER_1 = hp.HParam(\"conv_filter_1\", hp.Discrete([32,64,128]))\n",
    "HP_CONV_FILTER_2 = hp.HParam(\"conv_filter_2\", hp.Discrete([32,64,128]))\n",
    "HP_FC_UNITS = hp.HParam(\"fc_units\", hp.Discrete([30,60,120]))\n",
    "\n",
    "METRIC_ACCURACY = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-documentation",
   "metadata": {},
   "source": [
    "**Création et configuration des fichiers dans tensorboard**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/hparam_tuning_\" + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "with tf.summary.create_file_writer(log_dir).as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams = [HP_CONV_FILTER_1,\n",
    "                   HP_CONV_FILTER_2,\n",
    "                   HP_FC_UNITS\n",
    "                  ],\n",
    "        metrics = [hp.Metric(METRIC_ACCURACY,\n",
    "                             display_name=\"Accuracy\")\n",
    "                  ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-consultation",
   "metadata": {},
   "source": [
    "**Fonctions pour la modélisation avec gridsearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de modélisation avec les hyperparametres fournis\n",
    "\n",
    "def model(hparams) :\n",
    "    \n",
    "    model = Sequential([\n",
    "        Conv2D(filters = hparams[HP_CONV_FILTER_1],\n",
    "               kernel_size = (3, 3),\n",
    "               activation = 'relu',\n",
    "               padding = 'same',\n",
    "               input_shape = (IMG_HEIGHT,IMG_WIDTH,3)\n",
    "              ),\n",
    "        MaxPool2D(pool_size = (2, 2),\n",
    "                  strides=2\n",
    "                 ),\n",
    "        Conv2D(filters = hparams[HP_CONV_FILTER_2],\n",
    "               kernel_size=(3, 3),\n",
    "               activation = 'relu',\n",
    "               padding = 'same'\n",
    "              ),\n",
    "        MaxPool2D(pool_size = (2, 2),\n",
    "                  strides = 2\n",
    "                 ),\n",
    "        Flatten(),\n",
    "        Dense(units = hparams[HP_FC_UNITS],\n",
    "              activation = 'relu'\n",
    "             ),\n",
    "        Dense(units = N_BREEDS,\n",
    "              activation = 'softmax'\n",
    "             )\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # On compile\n",
    "    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0003),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy']\n",
    "                 )\n",
    "    \n",
    "    # Définir les callbacks\n",
    "    my_callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir),\n",
    "        hp.KerasCallback(log_dir, hparams),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", patience = 12),\n",
    "        tf.keras.callbacks.ModelCheckpoint(f\"model_VGG16_{N_BREEDS}_{'_'.join(str(x) for x in list(hparams.values()))}.hdf5\",\n",
    "                                           save_best_only=True,\n",
    "                                           monitor='val_loss',\n",
    "                                           mode='min')\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # On entraine, on récupere l'historique et on enregistre dans nos logs\n",
    "    history = model.fit(x = train_batches,\n",
    "                        steps_per_epoch = len(train_batches),\n",
    "                        validation_data = valid_batches,\n",
    "                        validation_steps = len(valid_batches),\n",
    "                        epochs = N_EPOCHS,\n",
    "                        verbose = 2,\n",
    "                        callbacks=my_callbacks\n",
    "                       )\n",
    "    \n",
    "    # On retourne la métrique d'optimisation\n",
    "    return history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'enregistrement pour chaque appel à notre fonction de modélisation, des hyperparametres utilisés et de la métrique\n",
    "\n",
    "def run(run_dir, hparams) :\n",
    "    \n",
    "    with tf.summary.create_file_writer(run_dir).as_default() :\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = model(hparams)\n",
    "        \n",
    "        # On convertit notre tensor métrique en scalaire\n",
    "        accuracy= tf.reshape(tf.convert_to_tensor(accuracy), []).numpy()\n",
    "        \n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher commande a executer pour tensorboard\n",
    "f\"python -m tensorboard.main --logdir=\\\"{log_dir}\\\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-highland",
   "metadata": {},
   "source": [
    "**Modélisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lance l'optimisation du modele avec notre espace d'hyperparametres\n",
    "\n",
    "session_num = 0\n",
    "\n",
    "for conv_filter_1 in HP_CONV_FILTER_1.domain.values:\n",
    "    for conv_filter_2 in HP_CONV_FILTER_2.domain.values:\n",
    "        for fc_units in HP_FC_UNITS.domain.values:\n",
    "            hparams = {\n",
    "                HP_CONV_FILTER_1: conv_filter_1,\n",
    "                HP_CONV_FILTER_2: conv_filter_2,\n",
    "                HP_FC_UNITS: fc_units,\n",
    "            }\n",
    "            run_name = f\"/run-{session_num}\"\n",
    "            run_dir = log_dir + run_name\n",
    "            print(f\"--- Starting trial:{run_name}\")\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run(run_dir, hparams)\n",
    "            session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-treaty",
   "metadata": {},
   "source": [
    "**Visualisation des résultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-cannon",
   "metadata": {},
   "source": [
    "ou pour afficher le tableau de bord en cours, on lance la commande suivante avec le chemin du répertoire où les différents journaux d'exécution ont été stockés:\n",
    "\n",
    "python -m tensorboard.main --logdir = log_dir\n",
    "\n",
    "et pour c\n",
    "taskkill /IM \"tensorboard.exe\" /F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-spread",
   "metadata": {},
   "source": [
    "**Analyse des résultats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20breeds = pd.read_csv(\"hparams_table_20breeds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20breeds[\"complexity\"] = df_3breeds[\"conv_filter_1\"]*df_3breeds[\"conv_filter_2\"]*df_3breeds[\"fc_units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_20breeds.sort_values(by=\"complexity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_20breeds, x=\"complexity\", y=\"Accuracy\", width=500, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_20breeds.groupby('conv_filter_1').mean(), y=\"Accuracy\", width=500, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_20breeds.groupby('conv_filter_2').mean(), y=\"Accuracy\", width=500, height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_20breeds.groupby('fc_units').mean(), y=\"Accuracy\", width=500, height=400)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
